# Mip-NeRF 360 Data Setup Guide
- Official Website: https://jonbarron.info/mipnerf360/
- Prepped Data
  - https://drive.google.com/drive/folders/1LbkkiywWdCH_FyGzJAA2CTo7HfEqMkVs?usp=sharing

## 1. Verify Your Dataset Structure
- dataset contains
```bash
/Mip-NeRF360_treehill/
│── poses_bounds.npy   # Stores camera poses & scene bounds (Generated by COLMAP)
│── /images/           # Full-resolution images
│── /images_2/         # Downsampled images (2x)
│── /images_4/         # Downsampled images (4x)
│── /images_8/         # Downsampled images (8x)
│── /sparse/           # COLMAP sparse reconstruction
```
### Prerequisites
- Install [Miniconda](https://docs.conda.io/en/latest/miniconda.html)
- Create conda environemnt

- Install required dependencies
```sh
pip install numpy json5 imageio tqdm
```

## 2. Create Python Script to Convert Poses
### A. Install Dependencies (If Not Installed)
- Make sure you're in your activated Conda environment (mipnerf360):
```sh
conda activate mipnerf360
```
- Then install required Python libraries
```sh
pip install numpy json5 imageio tqdm
```
### B. Create a Python Script for Conversion
- convert poses_bounds.npy to transforms.json
- create a new file named convert_poses.py.
- Code for convert_poses.py, copy and paste this into your script:
```python
import numpy as np
import json
import os

# Load the poses_bounds.npy file
data = np.load("poses_bounds.npy")  # Shape: (N, 17)
poses = data[:, :-2].reshape(-1, 3, 5)  # Extract camera poses
bounds = data[:, -2:]  # Near and far bounds

# Define camera intrinsics (modify if needed)
focal_length = poses[0, -1, 0]  # Focal length
camera_angle_x = 2 * np.arctan(poses[0, 2, 2] / (2 * focal_length))

# Convert to JSON format
frames = []
for i, pose in enumerate(poses):
    transform_matrix = np.eye(4)
    transform_matrix[:3, :] = pose[:, :4]  # Convert to 4x4 matrix

    frames.append({
        "file_path": f"images/img_{i:04d}.png",
        "transform_matrix": transform_matrix.tolist()
    })

# Save to transforms.json
transforms = {
    "camera_angle_x": camera_angle_x,
    "frames": frames
}

with open("transforms.json", "w") as f:
    json.dump(transforms, f, indent=4)

print("Conversion complete! Saved as transforms.json.")
```

### C. Run the Conversion Script
- In your dataset directory, run:
```sh
python convert_poses.py
```
- This will generate transforms.json in your dataset folder.

## 3. Train the Mip-NeRF 360 Model
- Once transforms.json is ready, you can start training.
### A. Install Training Dependencies
```sh
pip install gin-config imageio[ffmpeg] scipy matplotlib tqdm
```

### B. Run the Training Command
```sh
python -m train \
  --gin_configs=configs/360.gin \
  --gin_bindings="Config.data_dir = 'G:/YourDataDirectory'" \
  --gin_bindings="Config.checkpoint_dir = 'G:/My Drive/YouDataDirectory/checkpoints'" \
  --logtostderr
```

## 4. Render the Trained NeRF Model
- Once training is complete, generate a NeRF video
```sh
python -m render \
  --gin_configs=configs/360.gin \
  --gin_bindings="Config.data_dir = 'G:/My Drive/YourDataDirectory'" \
  --gin_bindings="Config.checkpoint_dir = 'G:/My Drive/YourDataDirectory/checkpoints'" \
  --gin_bindings="Config.render_dir = 'G:/My Drive/YourDataDirectory/render'" \
  --gin_bindings="Config.render_video_fps = 60" \
  --logtostderr
```
- Your NeRF video will be saved in:
```swift
G:/My Drive/YourDataDirectory/render/
```